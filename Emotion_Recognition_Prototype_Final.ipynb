{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion Recognition Prototype \u2014 Final (Colab-ready)\n",
        "This final notebook adds **Back Translation** (EN\u2192FR\u2192EN) and **Paraphrasing** (T5) to the previous prototype.\n\n",
        "**How to use:** Run cells top-to-bottom. When you reach the upload cell, upload `Dataset.csv`.\n",
        "Note: The transformer models will be downloaded into Colab on first run (may take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (runs in Colab)\n",
        "!pip install -q nltk scikit-learn matplotlib seaborn ipywidgets xgboost transformers datasets evaluate torch sentencepiece --upgrade\n",
        "print('Packages installed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np, nltk, string, random, time\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "print('NLTK ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload your dataset (CSV/XLSX). Run this cell and choose file when 'Choose Files' appears.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    dataset_path = fn\n",
        "print('Uploaded file:', dataset_path)\n",
        "df = pd.read_csv(dataset_path)\n",
        "print('Preview:')\n",
        "display(df.head())\n",
        "print('\\nColumns:', list(df.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Automatic suggestions for preprocessing\nRun the next cell to see quick recommendations based on your dataset content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def suggest_preprocessing(df):\n",
        "    suggestions = set()\n",
        "    text_cols = [c for c in df.columns if df[c].dtype == object]\n",
        "    if not text_cols:\n",
        "        return ['No text columns found']\n",
        "    txt = df[text_cols[0]].astype(str).str.cat(sep=' ')\n",
        "    if any(ch in string.punctuation for ch in txt):\n",
        "        suggestions.add('remove_punctuation')\n",
        "    if any(char.isdigit() for char in txt):\n",
        "        suggestions.add('remove_numbers')\n",
        "    from nltk.corpus import stopwords\n",
        "    sw = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(txt.lower())\n",
        "    stop_ratio = sum(1 for t in tokens if t in sw) / max(1, len(tokens))\n",
        "    if stop_ratio > 0.2:\n",
        "        suggestions.add('remove_stopwords')\n",
        "    if '  ' in txt:\n",
        "        suggestions.add('remove_extra_whitespace')\n",
        "    return list(suggestions)\n",
        "print('Auto-suggestions based on dataset:')\n",
        "print(suggest_preprocessing(df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Preprocessing functions (lowercase, punctuation, numbers, stopwords, whitespace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "def lowercase(text):\n",
        "    return text.lower() if isinstance(text, str) else text\n",
        "def remove_punctuation(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "def remove_numbers(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    return re.sub(r'\\d+', ' ', text)\n",
        "def remove_extra_whitespace(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    return ' '.join(text.split())\n",
        "def remove_stopwords(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join([t for t in tokens if t.lower() not in stop_words])\n",
        "def apply_preprocessing(text, steps):\n",
        "    funcs = {\n",
        "        'lowercase': lowercase,\n",
        "        'remove_punctuation': remove_punctuation,\n",
        "        'remove_numbers': remove_numbers,\n",
        "        'remove_extra_whitespace': remove_extra_whitespace,\n",
        "        'remove_stopwords': remove_stopwords\n",
        "    }\n",
        "    out = text\n",
        "    for s in steps:\n",
        "        f = funcs.get(s)\n",
        "        if f:\n",
        "            out = f(out)\n",
        "    return out\n",
        "print('Preprocessing functions ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Augmentation functions (synonym replacement, random insertion/swap/deletion, sentence shuffle/truncate, noise injection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "def get_synonyms(word):\n",
        "    syns = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for l in syn.lemmas():\n",
        "            w = l.name().replace('_',' ')\n",
        "            if w.lower() != word.lower():\n",
        "                syns.add(w)\n",
        "    return list(syns)\n",
        "from nltk.tokenize import word_tokenize\n",
        "def synonym_replacement(sentence, n=1):\n",
        "    words = word_tokenize(sentence)\n",
        "    eligible = [w for w in words if w.isalpha()]\n",
        "    random.shuffle(eligible)\n",
        "    num_replaced = 0\n",
        "    for w in eligible:\n",
        "        syns = get_synonyms(w)\n",
        "        if syns:\n",
        "            sentence = sentence.replace(w, random.choice(syns), 1)\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return sentence\n",
        "def random_deletion(sentence, p=0.1):\n",
        "    words = word_tokenize(sentence)\n",
        "    if len(words) == 1:\n",
        "        return sentence\n",
        "    remaining = [w for w in words if random.random() > p]\n",
        "    if len(remaining) == 0:\n",
        "        return random.choice(words)\n",
        "    return ' '.join(remaining)\n",
        "def random_swap(sentence, n=1):\n",
        "    words = word_tokenize(sentence)\n",
        "    if len(words) < 2: return sentence\n",
        "    for _ in range(n):\n",
        "        i, j = random.sample(range(len(words)), 2)\n",
        "        words[i], words[j] = words[j], words[i]\n",
        "    return ' '.join(words)\n",
        "def random_insertion(sentence, n=1):\n",
        "    words = word_tokenize(sentence)\n",
        "    for _ in range(n):\n",
        "        new_word = None\n",
        "        counter = 0\n",
        "        while new_word is None and counter < 10:\n",
        "            word = random.choice(words)\n",
        "            syns = get_synonyms(word)\n",
        "            if syns:\n",
        "                new_word = random.choice(syns)\n",
        "            counter += 1\n",
        "        if new_word:\n",
        "            pos = random.randint(0, len(words))\n",
        "            words.insert(pos, new_word)\n",
        "    return ' '.join(words)\n",
        "def sentence_shuffle(text):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    random.shuffle(sents)\n",
        "    return ' '.join(sents)\n",
        "def sentence_truncate(text, keep_ratio=0.7):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    k = max(1, int(len(sents)*keep_ratio))\n",
        "    return ' '.join(sents[:k])\n",
        "def noise_injection(text, p=0.05):\n",
        "    chars = list(text)\n",
        "    for i in range(len(chars)):\n",
        "        if random.random() < p:\n",
        "            chars[i] = random.choice(string.ascii_letters)\n",
        "    return ''.join(chars)\n",
        "print('Augmentation functions ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Back-translation (English -> French -> English) and Paraphrasing (T5)\nThese will download transformer models on first run. They may take 1\u20133 minutes to download depending on connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "print('Loading MarianMT models for back-translation...')\n",
        "start_time = time.time()\n",
        "tokenizer_en_fr = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
        "model_en_fr = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
        "tokenizer_fr_en = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
        "model_fr_en = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
        "print('Loading T5 model for paraphrasing...')\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
        "print('Models loaded in', round(time.time()-start_time,1), 's')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def back_translate(text, src_to_tgt_tokenizer, src_to_tgt_model, tgt_to_src_tokenizer, tgt_to_src_model, max_length=512):\n",
        "    # EN -> FR\n",
        "    inputs = src_to_tgt_tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    outs = src_to_tgt_model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "    mid = src_to_tgt_tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "    # FR -> EN\n",
        "    inputs2 = tgt_to_src_tokenizer.encode(mid, return_tensors='pt', truncation=True, max_length=512)\n",
        "    outs2 = tgt_to_src_model.generate(inputs2, max_length=512, num_beams=4, early_stopping=True)\n",
        "    back = tgt_to_src_tokenizer.decode(outs2[0], skip_special_tokens=True)\n",
        "    return back\n",
        "\n",
        "def paraphrase_t5(text, num_return_sequences=1, max_length=256):\n",
        "    # T5 paraphrase prefix\n",
        "    input_text = 'paraphrase: ' + text + ' </s>'\n",
        "    encoding = t5_tokenizer.encode_plus(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = t5_model.generate(encoding['input_ids'], attention_mask=encoding['attention_mask'],\n",
        "                                 max_length=max_length, num_beams=4, num_return_sequences=num_return_sequences, early_stopping=True)\n",
        "    paraphrases = [t5_tokenizer.decode(o, skip_special_tokens=True, clean_up_tokenization_spaces=True) for o in outputs]\n",
        "    return paraphrases[0] if paraphrases else text\n",
        "\n",
        "print('Back-translation & paraphrase functions ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## UI: choose text & label columns, suggest preprocessing, pick preprocessing/augmentation including back-translation/paraphrasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_col_dropdown = widgets.Dropdown(options=list(df.columns), description='Text col:')\n",
        "label_col_dropdown = widgets.Dropdown(options=list(df.columns), description='Label col:')\n",
        "suggest_btn = widgets.Button(description='Suggest Preprocessing')\n",
        "apply_prep_btn = widgets.Button(description='Apply Preprocessing')\n",
        "aug_choice = widgets.SelectMultiple(options=['synonym_replacement','random_insertion','random_swap','random_deletion','sentence_shuffle','sentence_truncate','noise_injection','back_translation','paraphrase_t5'],\n",
        "                                     description='Augment:')\n",
        "augment_btn = widgets.Button(description='Apply Augmentation (example)')\n",
        "out_ui = widgets.Output()\n",
        "lowercase_chk = widgets.Checkbox(value=True, description='Lowercase')\n",
        "punct_chk = widgets.Checkbox(value=True, description='Remove Punctuation')\n",
        "num_chk = widgets.Checkbox(value=True, description='Remove Numbers')\n",
        "stop_chk = widgets.Checkbox(value=True, description='Remove Stopwords')\n",
        "ws_chk = widgets.Checkbox(value=True, description='Extra Whitespace')\n",
        "\n",
        "def on_suggest(b):\n",
        "    with out_ui:\n",
        "        clear_output()\n",
        "        s = suggest_preprocessing(df)\n",
        "        print('Suggested steps:', s)\n",
        "suggest_btn.on_click(on_suggest)\n",
        "\n",
        "def on_apply_prep(b):\n",
        "    with out_ui:\n",
        "        clear_output()\n",
        "        tcol = text_col_dropdown.value\n",
        "        steps = []\n",
        "        if lowercase_chk.value: steps.append('lowercase')\n",
        "        if punct_chk.value: steps.append('remove_punctuation')\n",
        "        if num_chk.value: steps.append('remove_numbers')\n",
        "        if stop_chk.value: steps.append('remove_stopwords')\n",
        "        if ws_chk.value: steps.append('remove_extra_whitespace')\n",
        "        df['clean_text'] = df[tcol].astype(str).apply(lambda x: apply_preprocessing(x, steps))\n",
        "        print('Preprocessing applied to column', tcol)\n",
        "        display(df[[tcol,'clean_text']].head())\n",
        "apply_prep_btn.on_click(on_apply_prep)\n",
        "\n",
        "def on_augment(b):\n",
        "    with out_ui:\n",
        "        clear_output()\n",
        "        tcol = text_col_dropdown.value\n",
        "        choices = list(aug_choice.value)\n",
        "        # Apply examples and add columns\n",
        "        if 'synonym_replacement' in choices:\n",
        "            df['aug_syn'] = df[tcol].astype(str).apply(lambda x: synonym_replacement(x, n=1))\n",
        "        if 'random_insertion' in choices:\n",
        "            df['aug_ins'] = df[tcol].astype(str).apply(lambda x: random_insertion(x, n=1))\n",
        "        if 'random_swap' in choices:\n",
        "            df['aug_swap'] = df[tcol].astype(str).apply(lambda x: random_swap(x, n=1))\n",
        "        if 'random_deletion' in choices:\n",
        "            df['aug_del'] = df[tcol].astype(str).apply(lambda x: random_deletion(x, p=0.1))\n",
        "        if 'sentence_shuffle' in choices:\n",
        "            df['aug_shuf'] = df[tcol].astype(str).apply(lambda x: sentence_shuffle(x))\n",
        "        if 'sentence_truncate' in choices:\n",
        "            df['aug_trunc'] = df[tcol].astype(str).apply(lambda x: sentence_truncate(x, keep_ratio=0.7))\n",
        "        if 'noise_injection' in choices:\n",
        "            df['aug_noise'] = df[tcol].astype(str).apply(lambda x: noise_injection(x, p=0.03))\n",
        "        if 'back_translation' in choices:\n",
        "            # back-translate first 50 rows to avoid long runtime in demo\n",
        "            df['aug_bt'] = df[tcol].astype(str).head(50).apply(lambda x: back_translate(x, tokenizer_en_fr, model_en_fr, tokenizer_fr_en, model_fr_en))\n",
        "        if 'paraphrase_t5' in choices:\n",
        "            df['aug_para'] = df[tcol].astype(str).head(200).apply(lambda x: paraphrase_t5(x, num_return_sequences=1))\n",
        "        print('Augmentation columns added (examples).')\n",
        "        display(df.head())\n",
        "augment_btn.on_click(on_augment)\n",
        "\n",
        "display(text_col_dropdown, label_col_dropdown)\n",
        "display(widgets.HBox([lowercase_chk, punct_chk, num_chk, stop_chk, ws_chk]))\n",
        "display(suggest_btn, apply_prep_btn)\n",
        "display(widgets.HTML('<b>Choose augmentations (these create example augmented columns):</b>'))\n",
        "display(aug_choice, augment_btn, out_ui)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## Training and Evaluation\nChoose a model and click Train. DistilBERT option will fine-tune a transformer and can take longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_X_y():\n",
        "    tcol = text_col_dropdown.value\n",
        "    lcol = label_col_dropdown.value\n",
        "    if 'clean_text' in df.columns:\n",
        "        X = df['clean_text'].astype(str)\n",
        "    else:\n",
        "        X = df[tcol].astype(str)\n",
        "    y = df[lcol].astype(str)\n",
        "    return X, y\n",
        "\n",
        "model_options = ['Logistic Regression','Naive Bayes','SVM','XGBoost','DistilBERT (transformer)']\n",
        "model_dropdown = widgets.Dropdown(options=model_options, description='Model:')\n",
        "train_btn2 = widgets.Button(description='Train Model')\n",
        "out_train = widgets.Output()\n",
        "\n",
        "def train_classic_model(model_name, X_train, X_test, y_train, y_test):\n",
        "    vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "    Xtr = vect.fit_transform(X_train)\n",
        "    Xte = vect.transform(X_test)\n",
        "    if model_name == 'Logistic Regression':\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "    elif model_name == 'Naive Bayes':\n",
        "        model = MultinomialNB()\n",
        "    elif model_name == 'SVM':\n",
        "        model = LinearSVC()\n",
        "    else:\n",
        "        from xgboost import XGBClassifier\n",
        "        model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "    model.fit(Xtr, y_train)\n",
        "    y_pred = model.predict(Xte)\n",
        "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.show()\n",
        "    return model, vect\n",
        "\n",
        "def train_transformer(X_train, X_test, y_train, y_test, epochs=2):\n",
        "    from datasets import Dataset\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "    labels = sorted(list(set(y_train.tolist() + y_test.tolist())))\n",
        "    label2id = {l:i for i,l in enumerate(labels)}\n",
        "    def map_label(x):\n",
        "        return label2id[x]\n",
        "    train_ds = Dataset.from_dict({'text': X_train.tolist(), 'label':[label2id[l] for l in y_train.tolist()]})\n",
        "    test_ds = Dataset.from_dict({'text': X_test.tolist(), 'label':[label2id[l] for l in y_test.tolist()]})\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    def tokenize(batch): return tokenizer(batch['text'], truncation=True, padding=True, max_length=128)\n",
        "    train_ds = train_ds.map(tokenize, batched=True)\n",
        "    test_ds = test_ds.map(tokenize, batched=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))\n",
        "    training_args = TrainingArguments(output_dir='./results', per_device_train_batch_size=16, per_device_eval_batch_size=32, num_train_epochs=epochs, logging_steps=50, evaluation_strategy='epoch')\n",
        "    def compute_metrics(p):\n",
        "        preds = p.predictions.argmax(-1)\n",
        "        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "        acc = accuracy_score(p.label_ids, preds)\n",
        "        prf = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
        "        return {'accuracy': acc, 'precision': prf[0], 'recall': prf[1], 'f1': prf[2]}\n",
        "    trainer = Trainer(model=model, args=training_args, train_dataset=train_ds, eval_dataset=test_ds, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "    trainer.train()\n",
        "    eval_res = trainer.evaluate()\n",
        "    print(eval_res)\n",
        "\n",
        "def on_train_clicked2(b):\n",
        "    with out_train:\n",
        "        clear_output()\n",
        "        X, y = get_X_y()\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "        choice = model_dropdown.value\n",
        "        print('Training model:', choice)\n",
        "        if choice == 'DistilBERT (transformer)':\n",
        "            print('This will download transformer weights and may take time. Use small dataset for quick run.')\n",
        "            train_transformer(X_train, X_test, y_train, y_test, epochs=2)\n",
        "        else:\n",
        "            model, vect = train_classic_model(choice, X_train, X_test, y_train, y_test)\n",
        "            print('Done.')\n",
        "\n",
        "train_btn2.on_click(on_train_clicked2)\n",
        "display(model_dropdown, train_btn2, out_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n**Final notes:** Transformer-based back-translation and paraphrasing will download models on first run \u2014 please be patient. If the dataset is large, use `df = df.sample(2000)` to test faster."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}